#!/bin/sh

# tar up the files in parallel of a site, e.g. www.site.net -> www_site_net-assets-2-Tue.tar.gz

START=$(date +%s)

if [ -z $1 ]; then
  echo "Directory not provided"
  exit 1
fi

if [ -z $2 ]; then
  echo "S3 bucket/path not provided"
  exit 1
fi

if [ ! -d $1 ]; then
  echo "Not a directory: $1"
  exit 1
fi

SRCDIR=$(basename $1)
SRCPATH=$(dirname $1)
S3DST="s3://$2/${SRCDIR}/asset/"
FSDST="$(echo "$SRCDIR" | sed 's/\./_/g')-assets-$(date +%u-%a).tar.gz"
AWSCMD=/usr/local/bin/aws
PIGZ=/usr/local/bin/bacon

# log starting
echo "[$(date)] ${SRCDIR}: starting asset archive of ${SRCDIR} -> ${S3DST}${FSDST}"

# change to the directory so that it is 
cd "${SRCPATH}"

# make rw-r----
umask 0027

# specify the compress program as a wrapper for pigz so that we can control the number of threads
tar --use-compress-program=$PIGZ -cf "${FSDST}" "${SRCDIR}"
TAREXIT=$?

# see if tar had a bad exit
if [ "$TAREXIT" -gt 1 ]; then
	echo "tar had an error ($TAREXIT)"
	exit $TAREXIT
fi

echo "[$(date)] ${SRCDIR}: copying to ${S3DST}${FSDST}"

# send up to S3
$AWSCMD s3 cp --quiet --content-disposition="attachment; filename=\"${FSDST}\"" --sse=AES256 --storage-class=STANDARD_IA "${FSDST}" "${S3DST}"

# secondary standard storage, possibly transfered to cold
if [ ! -z "$3" ]; then
  FSSECDST="$(echo "$SRCDIR" | sed 's/\./_/g')-assets-$(date '+%Y%m%d%H%M%S').tar.gz"
  S3SECDST="s3://$3/assets/${SRCDIR}/$FSSECDST"

  $AWSCMD s3 cp --quiet --content-disposition="attachment; filename=\"${FSSECDST}\"" --sse=AES256 --storage-class=STANDARD "${FSDST}" "${S3SECDST}"
fi

TARSIZE=$(du -h "${FSDST}" | cut -f1)

# remove locally
rm ${FSDST}

# log finished
echo "[$(date)] ${SRCDIR}: finished, size ${TARSIZE}) in $(($(date +%s)-$START)) seconds"
